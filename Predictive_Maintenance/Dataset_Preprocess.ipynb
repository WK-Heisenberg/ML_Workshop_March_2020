{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess APS dataset\n",
    "\n",
    "#### In this notebook, we first download the data from UCI and preprocess it so we can build a Machine Learning model. \n",
    "#### We then store this data in a training and testing folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description:\n",
    "\n",
    "The dataset we use here for predictive maintenance comes from UCI Data Repository and consists of Air Pressure System failures recorded on Scania Trucks. Read more about the dataset here: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks\n",
    "\n",
    "The positive class consists of failures attributed to APS and negative class consists of failures in some other system. The goal is to identify APS failures correctly so a downstream predictive maintenance action can be taken on this system, once the origin of the failure has been identified.\n",
    "\n",
    "This is a typical use case in Predictive maintenance (PDM): a first model identifies the root cause of the failure. Once this is identified, a second system identifies how much time one has until a failure might occur which then informs the actions that need to be taken to avoid it. Predictive maintenance, like most machine learning problems can be multifaceted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install curl -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00421/aps_failure_training_set.csv --output aps_failure_training_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aps_failure_training_set.csv', sep=' ', encoding = 'utf-8', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this original dataset requires some preprocessing to get it in a suitable format for Machine learning. Run the function below to get a pre-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessdataset(df):\n",
    "    ''' Preprocess the input dataset for Machine learning training'''\n",
    "    \n",
    "    import os\n",
    "    try:\n",
    "        os.makedirs('training_data')\n",
    "    except Exception as e:\n",
    "        print(\"directory already exists\")\n",
    "        \n",
    "    try:\n",
    "        os.makedirs('test_data')\n",
    "    except Exception as e:\n",
    "        print(\"directory already exists\")\n",
    "    \n",
    "    print(\"Start Preprocessing ...\")\n",
    "    wholedf = pd.DataFrame(np.zeros(shape=(60000,171)), columns=np.arange(171))\n",
    "    wholedf.columns = df[0][14].split(',')\n",
    "    newdf = [df[0][row].split(',') for row in range(15 ,60015)]\n",
    "    newdf = pd.DataFrame.from_records(newdf)\n",
    "    newdf.columns = df[0][14].split(',')\n",
    "    \n",
    "    print(\"Dropping last 2 columns...\")\n",
    "    newdf = newdf.drop(columns = ['ef_000', 'eg_000'])\n",
    "    \n",
    "    print(\"Shape of the entire dataset ={}\".format(newdf.shape))\n",
    "    \n",
    "    print(\"Convert the class categorical label to numerical values for prediction\")\n",
    "    newdf = newdf.replace({'class': {'neg': 0, 'pos':1}})\n",
    "    newdf=newdf.replace('na',0)\n",
    "\n",
    "    print(\"Changing data types to numeric...\")\n",
    "    newdf = newdf.apply(pd.to_numeric)\n",
    "    \n",
    "    print(\"Splitting the data into train and test...\")\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test = train_test_split(newdf, test_size=0.2, random_state = 1234)\n",
    "    \n",
    "    print(\"Saving the data locally in train/test folders...\")\n",
    "    X_train.to_csv('training_data/train.csv', index = False, header = None)\n",
    "    X_test.to_csv('test_data/test.csv', index=False, header=None)\n",
    "    newdf.to_csv('rawdataset.csv', index=False, header=None)\n",
    "    print(\"Shape of Training data = {}\".format(X_train.shape))\n",
    "    print(\"Shape of Test data = {}\".format(X_test.shape))\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "preprocessdataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to \"predictive-maintenance-xgboost.ipynb\" and run the code cells to train your custom XGBoost model using SageMaker built in algorithms for predictive maintenance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
